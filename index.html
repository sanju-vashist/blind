<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <title>Unified Object & Text Detection with Camera Switch</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://cdn.jsdelivr.net/npm/tesseract.js@4.1.1/dist/tesseract.min.js"></script>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: #f5f7fa;
      color: #333;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
    }

    h2 {
      margin-bottom: 20px;
      color: #2c3e50;
      font-weight: 700;
      font-size: 2rem;
    }

    #container {
      display: flex;
      max-width: 960px;
      width: 100%;
      background: white;
      border-radius: 14px;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.15);
      overflow: hidden;
    }

    #video-section {
      flex: 2;
      background: #1e293b;
      padding: 20px;
      border-radius: 14px 0 0 14px;
      display: flex;
      flex-direction: column;
      align-items: center;
      position: relative;
    }

    #video-container {
      position: relative;
      width: 640px;
      height: 480px;
    }

    #webcam,
    #overlay {
      position: absolute;
      left: 0;
      top: 0;
      width: 640px;
      height: 480px;
      border-radius: 12px;
      object-fit: cover;
      display: block;
    }

    #cameraSelect {
      margin-top: 16px;
      font-size: 16px;
      width: 100%;
      max-width: 300px;
      padding: 8px 12px;
      border-radius: 7px;
      border: none;
      background: #2980b9;
      color: white;
      cursor: pointer;
      box-shadow: 0 4px 12px rgba(41, 128, 185, 0.5);
      transition: background-color 0.3s ease;
    }

    #cameraSelect:hover {
      background-color: #1f6391;
    }

    #status {
      margin-top: 14px;
      font-weight: 600;
      color: #e2e8f0;
      min-height: 24px;
      text-align: center;
      user-select: none;
      font-size: 1rem;
    }

    #sidebar {
      flex: 1;
      padding: 24px 20px;
      background: #f9fafb;
      border-left: 2px solid #d1d5db;
      max-height: 600px;
      overflow-y: auto;
      display: flex;
      flex-direction: column;
    }

    #sidebar h3 {
      font-size: 1.5rem;
      font-weight: 700;
      color: #4b5563;
      margin-bottom: 14px;
      text-align: center;
      text-transform: uppercase;
      letter-spacing: 0.05em;
    }

    #outputList {
      list-style: none;
      padding-left: 0;
      color: #2c3e50;
      font-weight: 600;
      font-size: 1.1rem;
      flex-grow: 1;
      overflow-y: auto;
    }

    #outputList li {
      background-color: #eaf1f8;
      border-radius: 12px;
      margin-bottom: 12px;
      padding: 12px 18px;
      box-shadow: inset 0 2px 4px #ffffffaa;
      display: flex;
      justify-content: space-between;
      align-items: center;
      letter-spacing: 0.02em;
      transition: background-color 0.3s;
      cursor: default;
    }

    #outputList li:hover {
      background-color: #c9d9f6;
      color: white;
    }

    #outputList li span.confidence {
      background-color: #2563eb;
      color: white;
      font-weight: 700;
      padding: 4px 12px;
      border-radius: 10px;
      font-size: 0.9rem;
      min-width: 50px;
      text-align: center;
    }

    @media (max-width: 850px) {
      #container {
        flex-direction: column;
        max-width: 90vw;
      }

      #video-section {
        border-radius: 14px 14px 0 0;
      }

      #sidebar {
        border-left: none;
        border-top: 2px solid #d1d5db;
        max-height: 250px;
        flex-direction: row;
        overflow-x: auto;
        padding: 12px 10px;
      }

      #sidebar h3 {
        display: none;
      }

      #outputList {
        display: flex;
        flex-direction: row;
        gap: 10px;
        font-size: 0.95rem;
        font-weight: 600;
      }

      #outputList li {
        min-width: 120px;
        white-space: nowrap;
      }
    }
  </style>
</head>

<body>
  <h2>Unified Object & Text Detection with Camera Switch</h2>
  <div id="container">
    <section id="video-section">
      <div id="video-container">
        <video id="webcam" autoplay muted playsinline></video>
        <canvas id="overlay"></canvas>
      </div>
      <select id="cameraSelect" aria-label="Select camera"></select>
      <div id="status">Loading models and initializing camera...</div>
    </section>
    <aside id="sidebar">
      <h3>Detected Items</h3>
      <ul id="outputList"><li>Loading detection results...</li></ul>
    </aside>
  </div>

  <script>
    const video = document.getElementById('webcam');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    const outputList = document.getElementById('outputList');
    const status = document.getElementById('status');
    const cameraSelect = document.getElementById('cameraSelect');

    let model = null;
    let stream = null;
    let lastOCRTime = 0;
    const OCR_INTERVAL = 3000;
    let previousItems = new Set();

    async function loadObjectModel() {
      status.textContent = "Loading object detection model...";
      return await cocoSsd.load();
    }

    async function populateCameraList() {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const videoDevices = devices.filter(device => device.kind === 'videoinput');
      cameraSelect.innerHTML = videoDevices.map((device, idx) =>
        `<option value="${device.deviceId}">${device.label || 'Camera ' + (idx + 1)}</option>`
      ).join('');
    }

    async function startCamera(deviceId) {
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }

      const constraints = { video: { deviceId: { exact: deviceId } } };
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = stream;

      await new Promise(resolve => {
        video.onloadedmetadata = () => {
          overlay.width = video.videoWidth;
          overlay.height = video.videoHeight;
          resolve();
        }
      });
    }

    async function runOCR() {
      const tempCanvas = document.createElement('canvas');
      tempCanvas.width = video.videoWidth;
      tempCanvas.height = video.videoHeight;
      const tCtx = tempCanvas.getContext('2d');
      tCtx.drawImage(video, 0, 0);

      const imgData = tCtx.getImageData(0, 0, tempCanvas.width, tempCanvas.height);
      for (let i = 0; i < imgData.data.length; i += 4) {
        const avg = (imgData.data[i] + imgData.data[i + 1] + imgData.data[i + 2]) / 3;
        const enhanced = avg > 128 ? Math.min(255, avg * 1.3) : avg * 0.7;
        imgData.data[i] = imgData.data[i + 1] = imgData.data[i + 2] = enhanced;
      }
      tCtx.putImageData(imgData, 0, 0);

      const dataUrl = tempCanvas.toDataURL();
      const { data: { text } } = await Tesseract.recognize(dataUrl, 'eng');

      return text.trim();
    }

    async function detectLoop() {
      ctx.clearRect(0, 0, overlay.width, overlay.height);

      if (!model || !video.videoWidth) {
        requestAnimationFrame(detectLoop);
        return;
      }

      // Run object detection
      const predictions = await model.detect(video);

      // Draw bounding boxes and labels
      predictions.forEach(pred => {
        const [x, y, w, h] = pred.bbox;
        ctx.strokeStyle = '#36a2eb';
        ctx.lineWidth = 3;
        ctx.fillStyle = 'rgba(54, 162, 235, 0.3)';
        ctx.fillRect(x, y, w, h);
        ctx.strokeRect(x, y, w, h);
        ctx.font = '18px Arial';
        ctx.fillStyle = '#153e75';
        ctx.fillText(`${pred.class} ${Math.round(pred.score * 100)}%`, x + 6, y > 20 ? y - 6 : y + 24);
      });

      // Aggregate detected items
      let detectedItems = new Set();
      predictions.forEach(p =>
        detectedItems.add(`${p.class} (${Math.round(p.score * 100)}%)`)
      );

      // Run OCR every OCR_INTERVAL ms
      if (Date.now() - lastOCRTime > OCR_INTERVAL) {
        lastOCRTime = Date.now();
        status.textContent = "Performing text recognition...";
        try {
          const textResult = await runOCR();
          if (textResult.length > 0) {
            const lines = textResult.split('\n').filter(l => l.trim() !== '');
            lines.forEach(line => detectedItems.add('Text: ' + line.trim()));
          }
          status.textContent = "Detection running...";
        } catch (e) {
          status.textContent = "OCR error: " + e.message;
        }
      } else {
        status.textContent = "Detection running...";
      }

      // Update sidebar and speech
      if (detectedItems.size === 0) {
        outputList.innerHTML = '<li>No objects or text detected.</li>';
      } else {
        const newItems = [...detectedItems].filter(i => !previousItems.has(i));
        if (newItems.length > 0) {
          const utterance = new SpeechSynthesisUtterance("I see " + newItems.join(', '));
          window.speechSynthesis.cancel();
          window.speechSynthesis.speak(utterance);
        }
        previousItems = detectedItems;
        outputList.innerHTML = '';
        detectedItems.forEach(i => {
          const li = document.createElement('li');
          if (i.startsWith('Text: ')) {
            li.textContent = i;
          } else {
            const splitIndex = i.lastIndexOf(' ');
            li.textContent = i.substring(0, splitIndex);
            const span = document.createElement('span');
            span.className = 'confidence';
            span.textContent = i.substring(splitIndex + 1);
            li.appendChild(span);
          }
          outputList.appendChild(li);
        });
      }

      requestAnimationFrame(detectLoop);
    }

    async function init() {
      status.textContent = "Loading model...";
      model = await loadObjectModel();

      status.textContent = "Loading cameras...";
      await populateCameraList();

      if (cameraSelect.options.length === 0) {
        status.textContent = "No cameras found.";
        return;
      }

      await startCamera(cameraSelect.value);
      detectLoop();
    }

    cameraSelect.onchange = async () => {
      status.textContent = "Switching camera...";
      await startCamera(cameraSelect.value);
      status.textContent = "Camera switched. Running detection...";
    };

    init();

  </script>
</body>

</html>
